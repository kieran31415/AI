{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi-v3\n",
    "\n",
    "Gym provides different game environments which we can plug into our code and train an agent with. The library takes care of the API for providing all the information that our agent requires, like possible actions, score, and current state. We just need to focus on the algorithm part of our agent.\n",
    "\n",
    "We'll be using the Gym environment called Taxi-v3, which all of the details explained in previous Notebook were pulled from. The objectives, rewards, and actions are all the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Gym(nasium)\n",
    "\n",
    "We need to install Gymnasium first (Gym from OpenAI is no longer supported). Docs: https://gymnasium.farama.org/  \n",
    "\n",
    "Executing the following should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.29.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (1.23.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[all] in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (4.4.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (1.23.3)\n",
      "Collecting mujoco-py<2.2,>=2.1\n",
      "  Using cached mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
      "Collecting lz4>=3.1.0\n",
      "  Using cached lz4-4.3.2-cp310-cp310-win_amd64.whl (99 kB)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (4.1.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (2.5.2)\n",
      "Collecting jaxlib>=0.4.0\n",
      "  Using cached jaxlib-0.4.18-cp310-cp310-win_amd64.whl (46.6 MB)\n",
      "Collecting mujoco>=2.3.3\n",
      "  Using cached mujoco-2.3.7-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "Collecting moviepy>=1.0.0\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Collecting imageio>=2.14.1\n",
      "  Using cached imageio-2.31.5-py3-none-any.whl (313 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (1.12.1)\n",
      "Collecting cython<3\n",
      "  Using cached Cython-0.29.36-py2.py3-none-any.whl (988 kB)\n",
      "Collecting box2d-py==2.3.5\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: opencv-python>=3.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (4.6.0.66)\n",
      "Collecting jax>=0.4.0\n",
      "  Using cached jax-0.4.18-py3-none-any.whl (1.7 MB)\n",
      "Collecting shimmy[atari]<1.0,>=0.1.0\n",
      "  Using cached Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[all]) (3.6.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imageio>=2.14.1->gymnasium[all]) (9.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.4.0->gymnasium[all]) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.4.0->gymnasium[all]) (1.9.2)\n",
      "Collecting ml-dtypes>=0.2.0\n",
      "  Using cached ml_dtypes-0.3.1-cp310-cp310-win_amd64.whl (126 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (4.37.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0->gymnasium[all]) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (4.64.1)\n",
      "Collecting proglog<=1.0.0\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[all]) (2.28.1)\n",
      "Collecting imageio-ffmpeg>=0.2.0\n",
      "  Using cached imageio_ffmpeg-0.4.9-py3-none-win_amd64.whl (22.6 MB)\n",
      "Requirement already satisfied: glfw in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mujoco>=2.3.3->gymnasium[all]) (2.5.5)\n",
      "Requirement already satisfied: pyopengl in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mujoco>=2.3.3->gymnasium[all]) (3.1.7)\n",
      "Requirement already satisfied: absl-py in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mujoco>=2.3.3->gymnasium[all]) (1.3.0)\n",
      "Collecting fasteners~=0.15\n",
      "  Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Collecting cffi>=1.10\n",
      "  Using cached cffi-1.16.0-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Collecting ale-py~=0.8.1\n",
      "  Using cached ale_py-0.8.1-cp310-cp310-win_amd64.whl (952 kB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-6.1.0-py3-none-any.whl (33 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[all]) (63.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy>=1.0.0->gymnasium[all]) (0.4.5)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n",
      "Installing collected packages: box2d-py, pycparser, mujoco, ml-dtypes, lz4, importlib-resources, imageio-ffmpeg, imageio, fasteners, decorator, cython, shimmy, proglog, jaxlib, jax, cffi, ale-py, mujoco-py, moviepy\n",
      "  Running setup.py install for box2d-py: started\n",
      "  Running setup.py install for box2d-py: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [28 lines of output]\n",
      "      Using setuptools (version 63.2.0).\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-310\n",
      "      creating build\\lib.win-amd64-cpython-310\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-310\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-310\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-310\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-310\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "      Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "      Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "      Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "      Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "      Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "      Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "      Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for box2d-py did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [30 lines of output]\n",
      "      Using setuptools (version 63.2.0).\n",
      "      running install\n",
      "      c:\\Users\\u0063152\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "        warnings.warn(\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-310\n",
      "      creating build\\lib.win-amd64-cpython-310\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-310\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-310\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-310\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-310\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "      Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "      Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "      Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "      Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "      Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "      Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "      Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> box2d-py\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[toy-text] in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[toy-text]) (1.23.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[toy-text]) (4.4.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[toy-text]) (2.2.0)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\u0063152\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[toy-text]) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium[toy-text]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gym's interface\n",
    "\n",
    "Once installed, we can load the game environment and render what it looks like. When we're all done, we can close the rendering an environment with env.close(). This will clear all memory. So only use when you want to start from scratch again.\n",
    "We can also print the action and state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just render the environment for 10 seconds, and close the window again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(6)\n",
      "State Space: Discrete(500)\n",
      "\n",
      "Current state: 111\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment\n",
    "state, info = env.reset() # reset to make sure with start with a randomly chosen start-state\n",
    "\n",
    "print(\"Action Space: {}\".format(env.action_space)) # 6 different actions we can take - discrete actions, no continuous actions (no 'range') \n",
    "print(\"State Space: {}\\n\".format(env.observation_space)) # 500 different states\n",
    "\n",
    "print(\"Current state: %d\" % env.unwrapped.s)\n",
    "\n",
    "env.render() # visualize the environment - we only need to call render once\n",
    "time.sleep(10)\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder of our problem:\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and dropoff locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination. In the illustration, these colors are reversed.\n",
    "\n",
    "- As verified by the prints, we have an Action Space of size 6 (south, north, east, west, pickup and dropoff) and a State Space of size 500 (the taxi's location x the passenger's location x the destination location).\n",
    "- The current state is randomly chosen (a state between 0 and 499)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Back to our illustration\n",
    "\n",
    "We can actually take our illustration, encode its state, and give it to the environment to render in Gym.\n",
    "\n",
    "<img src=\"./resources/taxi.png\" style=\"height: 350px\"/>\n",
    "\n",
    "Recall that we have the taxi at row 3, column 1, our passenger is at location 2 (=Y) (out of the 4 possible locations), and our destination is location 0 (=R). Using the Taxi-v3 state encoding method, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "# now, let's manually encode the state\n",
    "state = env.unwrapped.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger location, destination location)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.unwrapped.s = state\n",
    "\n",
    "env.render() # visualize the environment - we only need to call render once\n",
    "time.sleep(10)\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial State - Exercise\n",
    "\n",
    "Generate the state where the taxi is in the lower right corner. The passenger is in the taxi and the destination is B. What is the color of the taxi right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 499\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step-method\n",
    "\n",
    "The agent performs an action by using the step-method:\n",
    "\n",
    "```python\n",
    "state, reward, done, truncated, info = env.step(action)\n",
    "```\n",
    "\n",
    "Remember: There are 6 actions (0=south, 1=north, 2=east, 3=west, 4=pickup and 5=dropoff).\n",
    "\n",
    "The step-method returns:\n",
    "\n",
    "- state: the new state\n",
    "- reward: if your action was beneficial or not\n",
    "- done: indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
    "- truncated: is timelimit reached, or agent out of bounds?\n",
    "- info: additional info such as performance and latency for debugging purposes\n",
    "\n",
    "Let's go back to the illustrations state and try the different actions.\n",
    "\n",
    "<img src=\"./resources/taxi.png\" style=\"height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure we start in the position like the image, and let's take a step west (to the left). So, we'll hit a wall, and we should receive a penalty for not reaching the destination, and we should remain in the same state (because we hit the wall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 328\n",
      "New state: 328, reward: -1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment again\n",
    "state, info = env.reset() # reset to make sure with a clean env\n",
    "\n",
    "state = env.unwrapped.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"Initial state:\", state)\n",
    "env.unwrapped.s = state\n",
    "\n",
    "env.render() # visualize the environment - we only need to call render once\n",
    "time.sleep(10)\n",
    "state, reward, done, truncated, info = env.step(3) # go west\n",
    "time.sleep(10)\n",
    "env.close()\n",
    "\n",
    "print(\"New state: %d, reward: %d\" % (state, reward)) # a -1 penalty for every wall hit and the taxi won't move anywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do it again, but go north instead of west. So we won't hit a wall, but still, we haven't reached our destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 328\n",
      "New state: 228, reward: -1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment again\n",
    "state, info = env.reset() # reset to make sure with a clean env\n",
    "\n",
    "state = env.unwrapped.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"Initial state:\", state)\n",
    "env.unwrapped.s = state\n",
    "\n",
    "env.render() # visualize the environment - we only need to call render once\n",
    "time.sleep(10)\n",
    "state, reward, done, truncated, info = env.step(1) # go north\n",
    "time.sleep(10)\n",
    "done = True\n",
    "\n",
    "print(\"New state: %d, reward: %d\" % (state, reward)) #\n",
    "# taxi moves up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step-method - Exercise\n",
    "\n",
    "Generate the state again where the taxi is in the lower right corner. The passenger is in the taxi and the destination is B.\n",
    "\n",
    "Next, carry out the two steps: first drive the taxi to the destination location, and next drop off the passenger. \n",
    "Render the environment at each step, print the new state number, the reward and done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 499\n",
      "New state: 479, reward: -1, done: 0\n",
      "New state: 475, reward: 20, done: 1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Reward Table\n",
    "\n",
    "When the Taxi environment is created, there is an initial Reward table that's also created, called *P*. **We can think of it like a matrix that has the number of states as rows and the number of actions as columns, i.e. a states × actions matrix**.\n",
    "\n",
    "Since every state is in this matrix, we can see the default reward values assigned for state 479."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 479, -1, False)],\n",
       " 1: [(1.0, 379, -1, False)],\n",
       " 2: [(1.0, 499, -1, False)],\n",
       " 3: [(1.0, 479, -1, False)],\n",
       " 4: [(1.0, 479, -10, False)],\n",
       " 5: [(1.0, 475, 20, True)]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.s = 479\n",
    "env.unwrapped.P[479]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has the structure\n",
    "\n",
    "``\n",
    "{action: [(probability, nextstate, reward, done)]}.\n",
    "``\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "- The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state.\n",
    "- In this environment, the probability of each action is always 1.0.\n",
    "- The nextstate is the state we would be in if we take the action at this index of the dictionary.\n",
    "- All the movement actions have a -1 reward, the pickup action has a -10 reward, the dropoff actions has +20 reward in this particular state.\n",
    "- done is used to tell us when we have successfully dropped off a passenger at the right location (action 5 in this state). Each successful dropoff is the end of an episode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Solving the environment without Reinforcement Learning\n",
    "\n",
    "Let's see what will happen if we try to brute-force our way to solving the problem without RL.\n",
    "\n",
    "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The `env.action_space.sample()` method automatically selects one random action from the set of all possible actions.\n",
    "\n",
    "Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 200\n",
      "Penalties incurred: 62\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment again\n",
    "state, info = env.reset() # reset to make sure with a clean env\n",
    "\n",
    "env.unwrapped.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "    action = env.action_space.sample() \n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "\n",
    "    epochs += 1\n",
    "    finished = done or truncated\n",
    "env.close()\n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good. Our agent takes thousands (?) of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
    "\n",
    "This is because we aren't learning from past experience. We can run this over and over, and it will never optimize. **The agent has no memory of which action was best for each state**, which is exactly what Reinforcement Learning will do for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Q-learning\n",
    "\n",
    "We are going to use a *simple* RL algorithm called Q-learning which will give our agent some memory. Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "\n",
    "Therefore the Q-learning algorithm uses a Q-Table. The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). The matrix is first initialized to 0, and then values are updated during training.\n",
    "\n",
    "<img src=\"./resources/qtable.png\" style=\"height: 650px\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal action at every state is the action with the highest Q-value. So for state 328 the highest value is -1.971 (=North). For state 499 the highest value is 29 (=West). These actions indeed seem to be the best options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North is best?\n",
      "\n",
      "West is best?\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment again\n",
    "state, info = env.reset() # reset to make sure with a clean env\n",
    "\n",
    "env.unwrapped.s = 328\n",
    "env.render()\n",
    "print(\"North is best?\\n\")\n",
    "time.sleep(5)\n",
    "\n",
    "env.unwrapped.s = 499\n",
    "env.render()\n",
    "print(\"West is best?\")\n",
    "time.sleep(5)\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training the Agent\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the training algorithm that will update this Q-table as the agent explores the environment over 100 000 of episodes (of course you don't need to write this algorithm yourself).\n",
    "\n",
    "BTW: When we create the environment, we will not be rendering it, because this slows us down way too much. Especially in the next section, where we'll be training a q-table. After this training, we'll use the 'human' rendering again.\n",
    "\n",
    "\n",
    "Next, we'll initialize the Q-table to a 500×6 matrix of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: total: 1min 2s\n",
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state, info = env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "        next_state, reward, done, truncated, info = env.step(action) \n",
    "        finished = done or truncated\n",
    "\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Q-table has been established over 100 000 episodes, let's see what the Q-values are for our two states. Are North and West indeed the two most preferable moves for the two states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.4077079   -2.27325184  -2.41119495  -2.36186056 -10.68125864\n",
      " -11.00277739]\n",
      "[ 2.82744455  1.55412369  4.58255907 11.         -2.13330103 -2.81133522]\n"
     ]
    }
   ],
   "source": [
    "print(q_table[328])\n",
    "print(q_table[499])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluating the agent\n",
    "\n",
    "Let's evaluate the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value. Training is done, so no more balancing between exploration versus exploitation. Only exploit the knowledge gathered so far, in the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.95\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        action = np.argmax(q_table[state]) # take the action with the highest q-value\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        finished = done or truncated\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the evaluation that the agent's performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Solving the environment with Reinforcement Learning - Exercise\n",
    "\n",
    "Now that our agent is trained, we can use our human rendering again, and use our trained q-table to solve the taxi problem with 328 as the initial state (by analogy with section **7. Solving the environment without RL** of this notebook). Print the timesteps taken and penalties incurred. Pretty impressive! No?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 12\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Solving the environment with RL - Exercise\n",
    "\n",
    "Solve the problem for these two initial states.\n",
    "\n",
    "<img src=\"./resources/taxi1.png\" style=\"height: 300px\"/>\n",
    "<img src=\"./resources/taxi2.png\" style=\"height: 300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 32\n",
      "Timesteps taken: 15\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "# solve the problem with the first/second initial state\n",
    "# so encode the different states, like state = env.encode(row,column,passenger,destiny), and do the same thing: exploit the knowledge from the q-table: go for np.argmax(q_table[state])!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3762fa38335283772957b8ad4850e1d344741e862c3aa7ec1c26d17780db6975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
